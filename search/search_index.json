{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Penn Machine Learning Benchmarks This repository contains the code and data for a large, curated set of benchmark datasets for evaluating and comparing supervised machine learning algorithms. These data sets cover a broad range of applications, and include binary/multi-class classification problems and regression problems, as well as combinations of categorical, ordinal, and continuous features. There are no missing values in these data sets. Check the datasets directory for information about the individual data sets. Datasets Summary Data set format All data sets are stored in a common format: First row is the column names Each following row corresponds to one row of the data The target column is named target All columns are tab ( \\t ) separated All files are compressed with gzip to conserve space Citing PMLB If you use PMLB in a scientific publication, please consider citing the following paper: Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore (2017). PMLB: a large benchmark suite for machine learning evaluation and comparison . BioData Mining 10 , page 36. BibTeX entry: @article{Olson2017PMLB, author=\"Olson, Randal S. and La Cava, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.\", title=\"PMLB: a large benchmark suite for machine learning evaluation and comparison\", journal=\"BioData Mining\", year=\"2017\", month=\"Dec\", day=\"11\", volume=\"10\", number=\"1\", pages=\"36\", issn=\"1756-0381\", doi=\"10.1186/s13040-017-0154-4\", url=\"https://doi.org/10.1186/s13040-017-0154-4\" } Support for PMLB PMLB was developed in the Computational Genetics Lab at the University of Pennsylvania with funding from the NIH under grant R01 AI117694. We are incredibly grateful for the support of the NIH and the University of Pennsylvania during the development of this project.","title":"Home"},{"location":"#penn-machine-learning-benchmarks","text":"This repository contains the code and data for a large, curated set of benchmark datasets for evaluating and comparing supervised machine learning algorithms. These data sets cover a broad range of applications, and include binary/multi-class classification problems and regression problems, as well as combinations of categorical, ordinal, and continuous features. There are no missing values in these data sets. Check the datasets directory for information about the individual data sets.","title":"Penn Machine Learning Benchmarks"},{"location":"#datasets-summary","text":"","title":"Datasets Summary"},{"location":"#data-set-format","text":"All data sets are stored in a common format: First row is the column names Each following row corresponds to one row of the data The target column is named target All columns are tab ( \\t ) separated All files are compressed with gzip to conserve space","title":"Data set format"},{"location":"#citing-pmlb","text":"If you use PMLB in a scientific publication, please consider citing the following paper: Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore (2017). PMLB: a large benchmark suite for machine learning evaluation and comparison . BioData Mining 10 , page 36. BibTeX entry: @article{Olson2017PMLB, author=\"Olson, Randal S. and La Cava, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.\", title=\"PMLB: a large benchmark suite for machine learning evaluation and comparison\", journal=\"BioData Mining\", year=\"2017\", month=\"Dec\", day=\"11\", volume=\"10\", number=\"1\", pages=\"36\", issn=\"1756-0381\", doi=\"10.1186/s13040-017-0154-4\", url=\"https://doi.org/10.1186/s13040-017-0154-4\" }","title":"Citing PMLB"},{"location":"#support-for-pmlb","text":"PMLB was developed in the Computational Genetics Lab at the University of Pennsylvania with funding from the NIH under grant R01 AI117694. We are incredibly grateful for the support of the NIH and the University of Pennsylvania during the development of this project.","title":"Support for PMLB"},{"location":"contributing/","text":"Contribution Guides Introduction Thanks! First off, thank you for considering contributing to PMLB. We want this to be the easiest resource to use for benchmarking machine learning algorithms on many datasets. This is a community effort, and we rely on help from users like you. Why you should read this Making a really easy-to-use benchmark resource also means being diligent about how contributions are made. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, we will reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests. Types of contributions The main contribution our project needs at the moment is help identifying, sourcing and documenting the datasets that currently don't have that information. We would also consider dataset contributions that meet the format specifications of PMLB. We're open to other ideas (improving documentation, writing tutorials, etc.) that you may want to make. Ground Rules Be kind. We will, too. Responsibilities For sourcing/documentation of existing datasets, make sure your pull request follows our source guidelines For new datasets, make sure your pull request follows our new dataset guidelines Make sure your changes pass the tests. To check, please run the following (note, you must have the nose package installed within your dev environment for this to work): nosetests -s -v Create issues for any major changes and enhancements that you wish to make. Discuss things transparently and get community feedback. Be welcoming to newcomers and encourage diverse new contributors from all backgrounds. See the Python Community Code of Conduct as an example. Your First Contribution Help people who are new to your project understand where they can be most helpful. This is also a good time to let people know if you follow a label convention for flagging beginner issues. Unsure where to begin contributing to PMLB? You can start by looking through issues for help-wanted tags. If you haven't contributed to open source code before, check out these friendly tutorials: - http://makeapullrequest.com/ - http://www.firsttimersonly.com/ - How to Contribute to an Open Source Project on GitHub . Those guides should tell you everything you need to start out! Getting started How to submit a contribution Create your own fork of this repository Make the changes in your fork If you think the project would benefit from these changes: Make sure you have followed the guidelines above. Submit a pull request. How to report a bug When filing an issue, please make sure to answer these five questions: What version of PMLB are you using? What operating system and processor architecture are you using? What did you do? What did you expect to see? What did you see instead? Contributing Source Information Relevant issues: #13 , #22 . As part of our PMLB 2.0 Project , each dataset is getting revamped with a README file, a metadata.yaml file, and a summary statistics file. We need help doing this for each dataset! How to submit a contribution Verify the source for the dataset. Often the place to start is an internet search of the dataset name. Most datasets can be found in OpenML , the UC Irvine ML repository , or Kaggle . Follow \"How to verify source\" section to verify that the PMLB dataset actually came from the source you found. Update the information on the dataset's metadata.yaml file. Refer to the metadata template file or wine_quality_red as an example. Issue a pull request for your changes. In the pull request, document how you verified the source of the dataset, for example, by performing a checksum on the data. Include any information to help us independently check that what you have added is accurate. Contributing a new dataset New datasets should follow these guidelines: Each sample/observation forms a row of the dataset. Each feature/variable forms a column of the dataset. The dependent variable, i.e., outcome/target, should be labelled 'target' . If the task is classification, the dependent variable must be encoded with numeric, contiguous labels in [0, 1, .. k], where there are k classes in the data. Column headers are feature/variable names and 'target' . Any 'sample_id' or 'row_id' column should be excluded . The data should be tab-delimited and in .tsv.gz format. The dataset should be in the correct folder; i.e., for a classification dataset, penn-ml-benchmarks/datasets/classification/your_dataset/ A metadata.yaml file must be provided with all required fields filled in. Please follow the template guidelines. The dataset should not exceed 50 MB. Note that any pull requests for new dataset contributions will not be accepted if these guidelines are not met.","title":"Contributing"},{"location":"contributing/#contribution-guides","text":"","title":"Contribution Guides"},{"location":"contributing/#introduction","text":"","title":"Introduction"},{"location":"contributing/#thanks","text":"First off, thank you for considering contributing to PMLB. We want this to be the easiest resource to use for benchmarking machine learning algorithms on many datasets. This is a community effort, and we rely on help from users like you.","title":"Thanks!"},{"location":"contributing/#why-you-should-read-this","text":"Making a really easy-to-use benchmark resource also means being diligent about how contributions are made. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, we will reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests.","title":"Why you should read this"},{"location":"contributing/#types-of-contributions","text":"The main contribution our project needs at the moment is help identifying, sourcing and documenting the datasets that currently don't have that information. We would also consider dataset contributions that meet the format specifications of PMLB. We're open to other ideas (improving documentation, writing tutorials, etc.) that you may want to make.","title":"Types of contributions"},{"location":"contributing/#ground-rules","text":"","title":"Ground Rules"},{"location":"contributing/#be-kind","text":"We will, too.","title":"Be kind."},{"location":"contributing/#responsibilities","text":"For sourcing/documentation of existing datasets, make sure your pull request follows our source guidelines For new datasets, make sure your pull request follows our new dataset guidelines Make sure your changes pass the tests. To check, please run the following (note, you must have the nose package installed within your dev environment for this to work): nosetests -s -v Create issues for any major changes and enhancements that you wish to make. Discuss things transparently and get community feedback. Be welcoming to newcomers and encourage diverse new contributors from all backgrounds. See the Python Community Code of Conduct as an example.","title":"Responsibilities"},{"location":"contributing/#your-first-contribution","text":"Help people who are new to your project understand where they can be most helpful. This is also a good time to let people know if you follow a label convention for flagging beginner issues. Unsure where to begin contributing to PMLB? You can start by looking through issues for help-wanted tags. If you haven't contributed to open source code before, check out these friendly tutorials: - http://makeapullrequest.com/ - http://www.firsttimersonly.com/ - How to Contribute to an Open Source Project on GitHub . Those guides should tell you everything you need to start out!","title":"Your First Contribution"},{"location":"contributing/#getting-started","text":"","title":"Getting started"},{"location":"contributing/#how-to-submit-a-contribution","text":"Create your own fork of this repository Make the changes in your fork If you think the project would benefit from these changes: Make sure you have followed the guidelines above. Submit a pull request.","title":"How to submit a contribution"},{"location":"contributing/#how-to-report-a-bug","text":"When filing an issue, please make sure to answer these five questions: What version of PMLB are you using? What operating system and processor architecture are you using? What did you do? What did you expect to see? What did you see instead?","title":"How to report a bug"},{"location":"contributing/#contributing-source-information","text":"Relevant issues: #13 , #22 . As part of our PMLB 2.0 Project , each dataset is getting revamped with a README file, a metadata.yaml file, and a summary statistics file. We need help doing this for each dataset!","title":"Contributing Source Information"},{"location":"contributing/#how-to-submit-a-contribution_1","text":"Verify the source for the dataset. Often the place to start is an internet search of the dataset name. Most datasets can be found in OpenML , the UC Irvine ML repository , or Kaggle . Follow \"How to verify source\" section to verify that the PMLB dataset actually came from the source you found. Update the information on the dataset's metadata.yaml file. Refer to the metadata template file or wine_quality_red as an example. Issue a pull request for your changes. In the pull request, document how you verified the source of the dataset, for example, by performing a checksum on the data. Include any information to help us independently check that what you have added is accurate.","title":"How to submit a contribution"},{"location":"contributing/#contributing-a-new-dataset","text":"New datasets should follow these guidelines: Each sample/observation forms a row of the dataset. Each feature/variable forms a column of the dataset. The dependent variable, i.e., outcome/target, should be labelled 'target' . If the task is classification, the dependent variable must be encoded with numeric, contiguous labels in [0, 1, .. k], where there are k classes in the data. Column headers are feature/variable names and 'target' . Any 'sample_id' or 'row_id' column should be excluded . The data should be tab-delimited and in .tsv.gz format. The dataset should be in the correct folder; i.e., for a classification dataset, penn-ml-benchmarks/datasets/classification/your_dataset/ A metadata.yaml file must be provided with all required fields filled in. Please follow the template guidelines. The dataset should not exceed 50 MB. Note that any pull requests for new dataset contributions will not be accepted if these guidelines are not met.","title":"Contributing a new dataset"},{"location":"dataset/","text":"Dataset List Classification Benchmarks GAMETES_Epistasis_2_Way_1000atts_0.4H_EDM_1_EDM_1_1 GAMETES_Epistasis_2_Way_20atts_0.1H_EDM_1_1 GAMETES_Epistasis_2_Way_20atts_0.4H_EDM_1_1 GAMETES_Epistasis_3_Way_20atts_0.2H_EDM_1_1 GAMETES_Heterogeneity_20atts_1600_Het_0.4_0.2_50_EDM_2_001 GAMETES_Heterogeneity_20atts_1600_Het_0.4_0.2_75_EDM_2_001 Hill_Valley_with_noise Hill_Valley_without_noise adult agaricus_lepiota allbp allhyper allhypo allrep analcatdata_aids analcatdata_asbestos analcatdata_authorship analcatdata_bankruptcy analcatdata_boxing1 analcatdata_boxing2 analcatdata_creditscore analcatdata_cyyoung8092 analcatdata_cyyoung9302 analcatdata_dmft analcatdata_fraud analcatdata_germangss analcatdata_happiness analcatdata_japansolvent analcatdata_lawsuit ann_thyroid appendicitis australian auto backache balance_scale banana biomed breast breast_cancer breast_cancer_wisconsin breast_w buggyCrx bupa calendarDOW car car_evaluation cars cars1 chess churn clean1 clean2 cleve cleveland cleveland_nominal cloud cmc coil2000 colic collins confidence connect_4 contraceptive corral credit_a credit_g crx dermatology diabetes dis dna ecoli fars flags flare german glass glass2 haberman hayes_roth heart_c heart_h heart_statlog hepatitis horse_colic house_votes_84 hungarian hypothyroid ionosphere iris irish kddcup kr_vs_kp krkopt labor led24 led7 letter lupus lymphography magic mfeat_factors mfeat_fourier mfeat_karhunen mfeat_morphological mfeat_pixel mfeat_zernike mnist mofn_3_7_10 molecular_biology_promoters monk1 monk2 monk3 movement_libras mushroom mux6 new_thyroid nursery optdigits page_blocks parity5 parity5+5 pendigits phoneme pima poker postoperative_patient_data prnn_crabs prnn_fglass prnn_synth profb promoters ring saheart satimage schizo segmentation shuttle sleep solar_flare_1 solar_flare_2 sonar soybean spambase spect spectf splice tae texture threeOf9 tic_tac_toe titanic tokyo1 twonorm vehicle vote vowel waveform_21 waveform_40 wdbc wine_quality_red wine_quality_white wine_recognition xd6 yeast Regression Benchmarks 1027_ESL 1028_SWD 1029_LEV 1030_ERA 1089_USCrime 1096_FacultySalaries 1191_BNG_pbc 1193_BNG_lowbwt 1196_BNG_pharynx 1199_BNG_echoMonths 1201_BNG_breastTumor 1203_BNG_pwLinear 1595_poker 192_vineyard 195_auto_price 197_cpu_act 201_pol 207_autoPrice 210_cloud 215_2dplanes 218_house_8L 225_puma8NH 227_cpu_small 228_elusage 229_pwLinear 230_machine_cpu 294_satellite_image 344_mv 4544_GeographicalOriginalofMusic 485_analcatdata_vehicle 503_wind 505_tecator 519_vinnie 522_pm10 523_analcatdata_neavote 527_analcatdata_election2000 529_pollen 537_houses 542_pollution 547_no2 556_analcatdata_apnea2 557_analcatdata_apnea1 560_bodyfat 561_cpu 562_cpu_small 564_fried 573_cpu_act 574_house_16H 579_fri_c0_250_5 581_fri_c3_500_25 582_fri_c1_500_25 583_fri_c1_1000_50 584_fri_c4_500_25 586_fri_c3_1000_25 588_fri_c4_1000_100 589_fri_c2_1000_25 590_fri_c0_1000_50 591_fri_c1_100_10 592_fri_c4_1000_25 593_fri_c1_1000_10 594_fri_c2_100_5 595_fri_c0_1000_10 596_fri_c2_250_5 597_fri_c2_500_5 598_fri_c0_1000_25 599_fri_c2_1000_5 601_fri_c1_250_5 602_fri_c3_250_10 603_fri_c0_250_50 604_fri_c4_500_10 605_fri_c2_250_25 606_fri_c2_1000_10 607_fri_c4_1000_50 608_fri_c3_1000_10 609_fri_c0_1000_5 611_fri_c3_100_5 612_fri_c1_1000_5 613_fri_c3_250_5 615_fri_c4_250_10 616_fri_c4_500_50 617_fri_c3_500_5 618_fri_c3_1000_50 620_fri_c1_1000_25 621_fri_c0_100_10 622_fri_c2_1000_50 623_fri_c4_1000_10 624_fri_c0_100_5 626_fri_c2_500_50 627_fri_c2_500_10 628_fri_c3_1000_5 631_fri_c1_500_5 633_fri_c0_500_25 634_fri_c2_100_10 635_fri_c0_250_10 637_fri_c1_500_50 641_fri_c1_500_10 643_fri_c2_500_25 644_fri_c4_250_25 645_fri_c3_500_50 646_fri_c3_500_10 647_fri_c1_250_10 648_fri_c1_250_50 649_fri_c0_500_5 650_fri_c0_500_50 651_fri_c0_100_25 653_fri_c0_250_25 654_fri_c0_500_10 656_fri_c1_100_5 657_fri_c2_250_10 658_fri_c3_250_25 659_sleuth_ex1714 663_rabe_266 665_sleuth_case2002 666_rmftsa_ladata 678_visualizing_environmental 687_sleuth_ex1605 690_visualizing_galaxy 695_chatfield_4 706_sleuth_case1202 712_chscase_geyser1","title":"Dataset List"},{"location":"dataset/#dataset-list","text":"","title":"Dataset List"},{"location":"dataset/#classification-benchmarks","text":"GAMETES_Epistasis_2_Way_1000atts_0.4H_EDM_1_EDM_1_1 GAMETES_Epistasis_2_Way_20atts_0.1H_EDM_1_1 GAMETES_Epistasis_2_Way_20atts_0.4H_EDM_1_1 GAMETES_Epistasis_3_Way_20atts_0.2H_EDM_1_1 GAMETES_Heterogeneity_20atts_1600_Het_0.4_0.2_50_EDM_2_001 GAMETES_Heterogeneity_20atts_1600_Het_0.4_0.2_75_EDM_2_001 Hill_Valley_with_noise Hill_Valley_without_noise adult agaricus_lepiota allbp allhyper allhypo allrep analcatdata_aids analcatdata_asbestos analcatdata_authorship analcatdata_bankruptcy analcatdata_boxing1 analcatdata_boxing2 analcatdata_creditscore analcatdata_cyyoung8092 analcatdata_cyyoung9302 analcatdata_dmft analcatdata_fraud analcatdata_germangss analcatdata_happiness analcatdata_japansolvent analcatdata_lawsuit ann_thyroid appendicitis australian auto backache balance_scale banana biomed breast breast_cancer breast_cancer_wisconsin breast_w buggyCrx bupa calendarDOW car car_evaluation cars cars1 chess churn clean1 clean2 cleve cleveland cleveland_nominal cloud cmc coil2000 colic collins confidence connect_4 contraceptive corral credit_a credit_g crx dermatology diabetes dis dna ecoli fars flags flare german glass glass2 haberman hayes_roth heart_c heart_h heart_statlog hepatitis horse_colic house_votes_84 hungarian hypothyroid ionosphere iris irish kddcup kr_vs_kp krkopt labor led24 led7 letter lupus lymphography magic mfeat_factors mfeat_fourier mfeat_karhunen mfeat_morphological mfeat_pixel mfeat_zernike mnist mofn_3_7_10 molecular_biology_promoters monk1 monk2 monk3 movement_libras mushroom mux6 new_thyroid nursery optdigits page_blocks parity5 parity5+5 pendigits phoneme pima poker postoperative_patient_data prnn_crabs prnn_fglass prnn_synth profb promoters ring saheart satimage schizo segmentation shuttle sleep solar_flare_1 solar_flare_2 sonar soybean spambase spect spectf splice tae texture threeOf9 tic_tac_toe titanic tokyo1 twonorm vehicle vote vowel waveform_21 waveform_40 wdbc wine_quality_red wine_quality_white wine_recognition xd6 yeast","title":"Classification Benchmarks"},{"location":"dataset/#regression-benchmarks","text":"1027_ESL 1028_SWD 1029_LEV 1030_ERA 1089_USCrime 1096_FacultySalaries 1191_BNG_pbc 1193_BNG_lowbwt 1196_BNG_pharynx 1199_BNG_echoMonths 1201_BNG_breastTumor 1203_BNG_pwLinear 1595_poker 192_vineyard 195_auto_price 197_cpu_act 201_pol 207_autoPrice 210_cloud 215_2dplanes 218_house_8L 225_puma8NH 227_cpu_small 228_elusage 229_pwLinear 230_machine_cpu 294_satellite_image 344_mv 4544_GeographicalOriginalofMusic 485_analcatdata_vehicle 503_wind 505_tecator 519_vinnie 522_pm10 523_analcatdata_neavote 527_analcatdata_election2000 529_pollen 537_houses 542_pollution 547_no2 556_analcatdata_apnea2 557_analcatdata_apnea1 560_bodyfat 561_cpu 562_cpu_small 564_fried 573_cpu_act 574_house_16H 579_fri_c0_250_5 581_fri_c3_500_25 582_fri_c1_500_25 583_fri_c1_1000_50 584_fri_c4_500_25 586_fri_c3_1000_25 588_fri_c4_1000_100 589_fri_c2_1000_25 590_fri_c0_1000_50 591_fri_c1_100_10 592_fri_c4_1000_25 593_fri_c1_1000_10 594_fri_c2_100_5 595_fri_c0_1000_10 596_fri_c2_250_5 597_fri_c2_500_5 598_fri_c0_1000_25 599_fri_c2_1000_5 601_fri_c1_250_5 602_fri_c3_250_10 603_fri_c0_250_50 604_fri_c4_500_10 605_fri_c2_250_25 606_fri_c2_1000_10 607_fri_c4_1000_50 608_fri_c3_1000_10 609_fri_c0_1000_5 611_fri_c3_100_5 612_fri_c1_1000_5 613_fri_c3_250_5 615_fri_c4_250_10 616_fri_c4_500_50 617_fri_c3_500_5 618_fri_c3_1000_50 620_fri_c1_1000_25 621_fri_c0_100_10 622_fri_c2_1000_50 623_fri_c4_1000_10 624_fri_c0_100_5 626_fri_c2_500_50 627_fri_c2_500_10 628_fri_c3_1000_5 631_fri_c1_500_5 633_fri_c0_500_25 634_fri_c2_100_10 635_fri_c0_250_10 637_fri_c1_500_50 641_fri_c1_500_10 643_fri_c2_500_25 644_fri_c4_250_25 645_fri_c3_500_50 646_fri_c3_500_10 647_fri_c1_250_10 648_fri_c1_250_50 649_fri_c0_500_5 650_fri_c0_500_50 651_fri_c0_100_25 653_fri_c0_250_25 654_fri_c0_500_10 656_fri_c1_100_5 657_fri_c2_250_10 658_fri_c3_250_25 659_sleuth_ex1714 663_rabe_266 665_sleuth_case2002 666_rmftsa_ladata 678_visualizing_environmental 687_sleuth_ex1605 690_visualizing_galaxy 695_chatfield_4 706_sleuth_case1202 712_chscase_geyser1","title":"Regression Benchmarks"},{"location":"how-to-verify-source/","text":"How to verify source There are a few ways we can check whether a PMLB dataframe ( pmlb_df ) agrees with its source ( source_df ), provided that we have checked their shapes (by printing pmlb_df.shape and pmlb_df.shape ) and changed the column name of the dependent variable to target . For example, if the dependent variable in the source dataset is class , you can use df_source = df_source.rename(columns={'class': 'target'}) . If the two dataframes are exactly the same, the following line of code does not return anything \u2714\ufe0f: pd.testing.assert_frame_equal(df_source, df_pmlb) If it gives error, the column names may be different. If we have good reasons to ignore column names, we can check if the values contained in the 2 dataframes are the same with from pandas.util import hash_pandas_object import hashlib rowhashes_pmlb = hash_pandas_object(df_pmlb, index = False).values hash_pmlb = hashlib.sha256(rowhashes_pmlb).hexdigest() rowhashes_source = hash_pandas_object(df_source, index = False).values hash_source = hashlib.sha256(rowhashes_source).hexdigest() # verify hashes match print(hash_pmlb == hash_source) or (df_source.values == df_pmlb.values).all() If we still get False , it is possible that the rows have been permuted. To check if they are: set(df_pmlb.itertuples(index=False)) == set(df_source.itertuples(index=False)) or, if row hashes have been computed, set(rowhashes_source) == set(rowhashes_pmlb) or \"subtracting\" the two datasets row by row df_source.merge(df_pmlb, indicator = True, how='left').loc[lambda x : x['_merge']!='both'] df_pmlb.merge(df_source, indicator = True, how='left').loc[lambda x : x['_merge']!='both'] This code will print the rows that are in one dataframe but not the other and can help you see the difference a bit better. If the two dataframes have floats that are almost equal to each other, we can use numpy 's isclose to check if they are element-wise equal within a tolerance: from numpy import isclose isclose(df_source.values, df_pmlb.values).all() We have been using Google Colab notebooks to share our checks, but other methods are also welcomed. Here are a few existing notebooks for reference: wine-quality-red , wine-quality-white , waveform-mushroom-saheart , irish , adult .","title":"How to verify source"},{"location":"how-to-verify-source/#how-to-verify-source","text":"There are a few ways we can check whether a PMLB dataframe ( pmlb_df ) agrees with its source ( source_df ), provided that we have checked their shapes (by printing pmlb_df.shape and pmlb_df.shape ) and changed the column name of the dependent variable to target . For example, if the dependent variable in the source dataset is class , you can use df_source = df_source.rename(columns={'class': 'target'}) . If the two dataframes are exactly the same, the following line of code does not return anything \u2714\ufe0f: pd.testing.assert_frame_equal(df_source, df_pmlb) If it gives error, the column names may be different. If we have good reasons to ignore column names, we can check if the values contained in the 2 dataframes are the same with from pandas.util import hash_pandas_object import hashlib rowhashes_pmlb = hash_pandas_object(df_pmlb, index = False).values hash_pmlb = hashlib.sha256(rowhashes_pmlb).hexdigest() rowhashes_source = hash_pandas_object(df_source, index = False).values hash_source = hashlib.sha256(rowhashes_source).hexdigest() # verify hashes match print(hash_pmlb == hash_source) or (df_source.values == df_pmlb.values).all() If we still get False , it is possible that the rows have been permuted. To check if they are: set(df_pmlb.itertuples(index=False)) == set(df_source.itertuples(index=False)) or, if row hashes have been computed, set(rowhashes_source) == set(rowhashes_pmlb) or \"subtracting\" the two datasets row by row df_source.merge(df_pmlb, indicator = True, how='left').loc[lambda x : x['_merge']!='both'] df_pmlb.merge(df_source, indicator = True, how='left').loc[lambda x : x['_merge']!='both'] This code will print the rows that are in one dataframe but not the other and can help you see the difference a bit better. If the two dataframes have floats that are almost equal to each other, we can use numpy 's isclose to check if they are element-wise equal within a tolerance: from numpy import isclose isclose(df_source.values, df_pmlb.values).all() We have been using Google Colab notebooks to share our checks, but other methods are also welcomed. Here are a few existing notebooks for reference: wine-quality-red , wine-quality-white , waveform-mushroom-saheart , irish , adult .","title":"How to verify source"},{"location":"installing/","text":"Installation of Python wrapper For easy access to the benchmark data sets, we have provided a Python wrapper named pmlb . The wrapper can be installed on Python via pip : pip install pmlb","title":"Installation"},{"location":"installing/#installation-of-python-wrapper","text":"For easy access to the benchmark data sets, we have provided a Python wrapper named pmlb . The wrapper can be installed on Python via pip : pip install pmlb","title":"Installation of Python wrapper"},{"location":"using/","text":"Using PMLB Using PMLB in Python scripts from pmlb import fetch_data # Returns a pandas DataFrame adult_data = fetch_data('adult') print(adult_data.describe()) The fetch_data function has two additional parameters: * return_X_y (True/False): Whether to return the data in scikit-learn format, with the features and labels stored in separate NumPy arrays. * local_cache_dir (string): The directory on your local machine to store the data files so you don't have to fetch them over the web again. By default, the wrapper does not use a local cache directory. For example: from pmlb import fetch_data # Returns NumPy arrays adult_X, adult_y = fetch_data('adult', return_X_y=True, local_cache_dir='./') print(adult_X) print(adult_y) You can also list all of the available data sets as follows: from pmlb import dataset_names print(dataset_names) Or if you only want a list of available classification or regression datasets: from pmlb import classification_dataset_names, regression_dataset_names print(classification_dataset_names) print('') print(regression_dataset_names) Example usage: Compare two classification algorithms with PMLB PMLB is designed to make it easy to benchmark machine learning algorithms against each other. Below is a Python code snippet showing the most basic way to use PMLB to compare two algorithms. from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sb from pmlb import fetch_data, classification_dataset_names logit_test_scores = [] gnb_test_scores = [] for classification_dataset in classification_dataset_names: X, y = fetch_data(classification_dataset, return_X_y=True) train_X, test_X, train_y, test_y = train_test_split(X, y) logit = LogisticRegression() gnb = GaussianNB() logit.fit(train_X, train_y) gnb.fit(train_X, train_y) logit_test_scores.append(logit.score(test_X, test_y)) gnb_test_scores.append(gnb.score(test_X, test_y)) sb.boxplot(data=[logit_test_scores, gnb_test_scores], notch=True) plt.xticks([0, 1], ['LogisticRegression', 'GaussianNB']) plt.ylabel('Test Accuracy')","title":"Using PMLB python wrapper"},{"location":"using/#using-pmlb","text":"","title":"Using PMLB"},{"location":"using/#using-pmlb-in-python-scripts","text":"from pmlb import fetch_data # Returns a pandas DataFrame adult_data = fetch_data('adult') print(adult_data.describe()) The fetch_data function has two additional parameters: * return_X_y (True/False): Whether to return the data in scikit-learn format, with the features and labels stored in separate NumPy arrays. * local_cache_dir (string): The directory on your local machine to store the data files so you don't have to fetch them over the web again. By default, the wrapper does not use a local cache directory. For example: from pmlb import fetch_data # Returns NumPy arrays adult_X, adult_y = fetch_data('adult', return_X_y=True, local_cache_dir='./') print(adult_X) print(adult_y) You can also list all of the available data sets as follows: from pmlb import dataset_names print(dataset_names) Or if you only want a list of available classification or regression datasets: from pmlb import classification_dataset_names, regression_dataset_names print(classification_dataset_names) print('') print(regression_dataset_names)","title":"Using PMLB in Python scripts"},{"location":"using/#example-usage-compare-two-classification-algorithms-with-pmlb","text":"PMLB is designed to make it easy to benchmark machine learning algorithms against each other. Below is a Python code snippet showing the most basic way to use PMLB to compare two algorithms. from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sb from pmlb import fetch_data, classification_dataset_names logit_test_scores = [] gnb_test_scores = [] for classification_dataset in classification_dataset_names: X, y = fetch_data(classification_dataset, return_X_y=True) train_X, test_X, train_y, test_y = train_test_split(X, y) logit = LogisticRegression() gnb = GaussianNB() logit.fit(train_X, train_y) gnb.fit(train_X, train_y) logit_test_scores.append(logit.score(test_X, test_y)) gnb_test_scores.append(gnb.score(test_X, test_y)) sb.boxplot(data=[logit_test_scores, gnb_test_scores], notch=True) plt.xticks([0, 1], ['LogisticRegression', 'GaussianNB']) plt.ylabel('Test Accuracy')","title":"Example usage: Compare two classification algorithms with PMLB"}]}