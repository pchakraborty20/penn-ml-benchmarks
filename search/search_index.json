{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Penn Machine Learning Benchmarks This repository contains the code and data for a large, curated set of benchmark datasets for evaluating and comparing supervised machine learning algorithms. These data sets cover a broad range of applications, and include binary/multi-class classification problems and regression problems, as well as combinations of categorical, ordinal, and continuous features. There are no missing values in these data sets. Check the datasets directory for information about the individual data sets. Datasets Summary Data set format All data sets are stored in a common format: First row is the column names Each following row corresponds to one row of the data The target column is named target All columns are tab ( \\t ) separated All files are compressed with gzip to conserve space Citing PMLB If you use PMLB in a scientific publication, please consider citing the following paper: Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore (2017). PMLB: a large benchmark suite for machine learning evaluation and comparison . BioData Mining 10 , page 36. BibTeX entry: @article{Olson2017PMLB, author=\"Olson, Randal S. and La Cava, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.\", title=\"PMLB: a large benchmark suite for machine learning evaluation and comparison\", journal=\"BioData Mining\", year=\"2017\", month=\"Dec\", day=\"11\", volume=\"10\", number=\"1\", pages=\"36\", issn=\"1756-0381\", doi=\"10.1186/s13040-017-0154-4\", url=\"https://doi.org/10.1186/s13040-017-0154-4\" } Support for PMLB PMLB was developed in the Computational Genetics Lab at the University of Pennsylvania with funding from the NIH under grant R01 AI117694. We are incredibly grateful for the support of the NIH and the University of Pennsylvania during the development of this project.","title":"Home"},{"location":"#penn-machine-learning-benchmarks","text":"This repository contains the code and data for a large, curated set of benchmark datasets for evaluating and comparing supervised machine learning algorithms. These data sets cover a broad range of applications, and include binary/multi-class classification problems and regression problems, as well as combinations of categorical, ordinal, and continuous features. There are no missing values in these data sets. Check the datasets directory for information about the individual data sets.","title":"Penn Machine Learning Benchmarks"},{"location":"#datasets-summary","text":"","title":"Datasets Summary"},{"location":"#data-set-format","text":"All data sets are stored in a common format: First row is the column names Each following row corresponds to one row of the data The target column is named target All columns are tab ( \\t ) separated All files are compressed with gzip to conserve space","title":"Data set format"},{"location":"#citing-pmlb","text":"If you use PMLB in a scientific publication, please consider citing the following paper: Randal S. Olson, William La Cava, Patryk Orzechowski, Ryan J. Urbanowicz, and Jason H. Moore (2017). PMLB: a large benchmark suite for machine learning evaluation and comparison . BioData Mining 10 , page 36. BibTeX entry: @article{Olson2017PMLB, author=\"Olson, Randal S. and La Cava, William and Orzechowski, Patryk and Urbanowicz, Ryan J. and Moore, Jason H.\", title=\"PMLB: a large benchmark suite for machine learning evaluation and comparison\", journal=\"BioData Mining\", year=\"2017\", month=\"Dec\", day=\"11\", volume=\"10\", number=\"1\", pages=\"36\", issn=\"1756-0381\", doi=\"10.1186/s13040-017-0154-4\", url=\"https://doi.org/10.1186/s13040-017-0154-4\" }","title":"Citing PMLB"},{"location":"#support-for-pmlb","text":"PMLB was developed in the Computational Genetics Lab at the University of Pennsylvania with funding from the NIH under grant R01 AI117694. We are incredibly grateful for the support of the NIH and the University of Pennsylvania during the development of this project.","title":"Support for PMLB"},{"location":"contributing/","text":"Introduction Thanks! First off, thank you for considering contributing to PMLB. We want this to be the easiest resource to use for benchmarking machine learning algorithms on many datasets. This is a community effort, and we rely on help from users like you. Why you should read this Making a really easy-to-use benchmark resource also means being diligent about how contributions are made. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, we will reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests. Types of contributions The main contribution our project needs at the moment is help identifying, sourcing and documenting the datasets that currently don't have that information. We would also consider dataset contributions that meet the format specifications of PMLB. We're open to other ideas (improving documentation, writing tutorials, etc.) that you may want to make. Ground Rules Be kind. We will, too. Responsibilities For sourcing/documentation of existing datasets, make sure your pull request follows our source guidelines For new datasets, make sure your pull request follows our new dataset guidelines Make sure your changes pass the tests. To check, please run the following (note, you must have the nose package installed within your dev environment for this to work): nosetests -s -v Create issues for any major changes and enhancements that you wish to make. Discuss things transparently and get community feedback. Be welcoming to newcomers and encourage diverse new contributors from all backgrounds. See the Python Community Code of Conduct as an example. Your First Contribution Help people who are new to your project understand where they can be most helpful. This is also a good time to let people know if you follow a label convention for flagging beginner issues. Unsure where to begin contributing to PMLB? You can start by looking through issues for help-wanted tags. If you haven't contributed to open source code before, check out these friendly tutorials: - http://makeapullrequest.com/ - http://www.firsttimersonly.com/ - How to Contribute to an Open Source Project on GitHub . Those guides should tell you everything you need to start out! Getting started How to submit a contribution Create your own fork of this repository Make the changes in your fork If you think the project would benefit from these changes: Make sure you have followed the guidelines above. Submit a pull request. How to report a bug When filing an issue, please make sure to answer these five questions: What version of PMLB are you using? What operating system and processor architecture are you using? What did you do? What did you expect to see? What did you see instead? Contributing Source Information Relevant issues: #13 , #22 . As part of our PMLB 2.0 Project , each dataset is getting revamped with a README file, a metadata.yaml file, and a summary statistics file. We need help doing this for each dataset! How to submit a contribution Verify the source for the dataset. Often the place to start is an internet search of the dataset name. Most datasets can be found in OpenML , the UC Irvine ML repository , or Kaggle . Follow \"How to verify source\" section to verify that the PMLB dataset actually came from the source you found. Update the information on the dataset's metadata.yaml file. Refer to the metadata template file or wine_quality_red as an example. Issue a pull request for your changes. In the pull request, document how you verified the source of the dataset, for example, by performing a checksum on the data. Include any information to help us independently check that what you have added is accurate. Contributing a new dataset New datasets should follow these guidelines: Each sample/observation forms a row of the dataset. Each feature/variable forms a column of the dataset. The dependent variable, i.e., outcome/target, should be labelled 'target' . If the task is classification, the dependent variable must be encoded with numeric, contiguous labels in [0, 1, .. k], where there are k classes in the data. Column headers are feature/variable names and 'target' . Any 'sample_id' or 'row_id' column should be excluded . The data should be tab-delimited and in .tsv.gz format. The dataset should be in the correct folder; i.e., for a classification dataset, penn-ml-benchmarks/datasets/classification/your_dataset/ A metadata.yaml file must be provided with all required fields filled in. Please follow the template guidelines. The dataset should not exceed 50 MB. Note that any pull requests for new dataset contributions will not be accepted if these guidelines are not met.","title":"Contributing"},{"location":"contributing/#introduction","text":"","title":"Introduction"},{"location":"contributing/#thanks","text":"First off, thank you for considering contributing to PMLB. We want this to be the easiest resource to use for benchmarking machine learning algorithms on many datasets. This is a community effort, and we rely on help from users like you.","title":"Thanks!"},{"location":"contributing/#why-you-should-read-this","text":"Making a really easy-to-use benchmark resource also means being diligent about how contributions are made. Following these guidelines helps to communicate that you respect the time of the developers managing and developing this open source project. In return, we will reciprocate that respect in addressing your issue, assessing changes, and helping you finalize your pull requests.","title":"Why you should read this"},{"location":"contributing/#types-of-contributions","text":"The main contribution our project needs at the moment is help identifying, sourcing and documenting the datasets that currently don't have that information. We would also consider dataset contributions that meet the format specifications of PMLB. We're open to other ideas (improving documentation, writing tutorials, etc.) that you may want to make.","title":"Types of contributions"},{"location":"contributing/#ground-rules","text":"","title":"Ground Rules"},{"location":"contributing/#be-kind","text":"We will, too.","title":"Be kind."},{"location":"contributing/#responsibilities","text":"For sourcing/documentation of existing datasets, make sure your pull request follows our source guidelines For new datasets, make sure your pull request follows our new dataset guidelines Make sure your changes pass the tests. To check, please run the following (note, you must have the nose package installed within your dev environment for this to work): nosetests -s -v Create issues for any major changes and enhancements that you wish to make. Discuss things transparently and get community feedback. Be welcoming to newcomers and encourage diverse new contributors from all backgrounds. See the Python Community Code of Conduct as an example.","title":"Responsibilities"},{"location":"contributing/#your-first-contribution","text":"Help people who are new to your project understand where they can be most helpful. This is also a good time to let people know if you follow a label convention for flagging beginner issues. Unsure where to begin contributing to PMLB? You can start by looking through issues for help-wanted tags. If you haven't contributed to open source code before, check out these friendly tutorials: - http://makeapullrequest.com/ - http://www.firsttimersonly.com/ - How to Contribute to an Open Source Project on GitHub . Those guides should tell you everything you need to start out!","title":"Your First Contribution"},{"location":"contributing/#getting-started","text":"","title":"Getting started"},{"location":"contributing/#how-to-submit-a-contribution","text":"Create your own fork of this repository Make the changes in your fork If you think the project would benefit from these changes: Make sure you have followed the guidelines above. Submit a pull request.","title":"How to submit a contribution"},{"location":"contributing/#how-to-report-a-bug","text":"When filing an issue, please make sure to answer these five questions: What version of PMLB are you using? What operating system and processor architecture are you using? What did you do? What did you expect to see? What did you see instead?","title":"How to report a bug"},{"location":"contributing/#contributing-source-information","text":"Relevant issues: #13 , #22 . As part of our PMLB 2.0 Project , each dataset is getting revamped with a README file, a metadata.yaml file, and a summary statistics file. We need help doing this for each dataset!","title":"Contributing Source Information"},{"location":"contributing/#how-to-submit-a-contribution_1","text":"Verify the source for the dataset. Often the place to start is an internet search of the dataset name. Most datasets can be found in OpenML , the UC Irvine ML repository , or Kaggle . Follow \"How to verify source\" section to verify that the PMLB dataset actually came from the source you found. Update the information on the dataset's metadata.yaml file. Refer to the metadata template file or wine_quality_red as an example. Issue a pull request for your changes. In the pull request, document how you verified the source of the dataset, for example, by performing a checksum on the data. Include any information to help us independently check that what you have added is accurate.","title":"How to submit a contribution"},{"location":"contributing/#contributing-a-new-dataset","text":"New datasets should follow these guidelines: Each sample/observation forms a row of the dataset. Each feature/variable forms a column of the dataset. The dependent variable, i.e., outcome/target, should be labelled 'target' . If the task is classification, the dependent variable must be encoded with numeric, contiguous labels in [0, 1, .. k], where there are k classes in the data. Column headers are feature/variable names and 'target' . Any 'sample_id' or 'row_id' column should be excluded . The data should be tab-delimited and in .tsv.gz format. The dataset should be in the correct folder; i.e., for a classification dataset, penn-ml-benchmarks/datasets/classification/your_dataset/ A metadata.yaml file must be provided with all required fields filled in. Please follow the template guidelines. The dataset should not exceed 50 MB. Note that any pull requests for new dataset contributions will not be accepted if these guidelines are not met.","title":"Contributing a new dataset"},{"location":"how-to-verify-source/","text":"There are a few ways we can check whether a PMLB dataframe ( pmlb_df ) agrees with its source ( source_df ), provided that we have checked their shapes (by printing pmlb_df.shape and pmlb_df.shape ) and changed the column name of the dependent variable to target . For example, if the dependent variable in the source dataset is class , you can use df_source = df_source.rename(columns={'class': 'target'}) . If the two dataframes are exactly the same, the following line of code does not return anything \u2714\ufe0f: python pd.testing.assert_frame_equal(df_source, df_pmlb) If it gives error, the column names may be different. If we have good reasons to ignore column names, we can check if the values contained in the 2 dataframes are the same with ```python from pandas.util import hash_pandas_object import hashlib rowhashes_pmlb = hash_pandas_object(df_pmlb, index = False).values hash_pmlb = hashlib.sha256(rowhashes_pmlb).hexdigest() rowhashes_source = hash_pandas_object(df_source, index = False).values hash_source = hashlib.sha256(rowhashes_source).hexdigest() # verify hashes match print(hash_pmlb == hash_source) ``` or python (df_source.values == df_pmlb.values).all() - If we still get False , it is possible that the rows have been permuted. To check if they are: python set(df_pmlb.itertuples(index=False)) == set(df_source.itertuples(index=False)) or, if row hashes have been computed, python set(rowhashes_source) == set(rowhashes_pmlb) or \"subtracting\" the two datasets row by row python df_source.merge(df_pmlb, indicator = True, how='left').loc[lambda x : x['_merge']!='both'] df_pmlb.merge(df_source, indicator = True, how='left').loc[lambda x : x['_merge']!='both'] This code will print the rows that are in one dataframe but not the other and can help you see the difference a bit better. - If the two dataframes have floats that are almost equal to each other, we can use numpy 's isclose to check if they are element-wise equal within a tolerance: ```python from numpy import isclose isclose(df_source.values, df_pmlb.values).all() ``` We have been using Google Colab notebooks to share our checks, but other methods are also welcomed. Here are a few existing notebooks for reference: wine-quality-red , wine-quality-white , waveform-mushroom-saheart , irish , adult .","title":"How to verify source"},{"location":"installing/","text":"Installation of Python wrapper For easy access to the benchmark data sets, we have provided a Python wrapper named pmlb . The wrapper can be installed on Python via pip : pip install pmlb","title":"Installation"},{"location":"installing/#installation-of-python-wrapper","text":"For easy access to the benchmark data sets, we have provided a Python wrapper named pmlb . The wrapper can be installed on Python via pip : pip install pmlb","title":"Installation of Python wrapper"},{"location":"using/","text":"Using PMLB in Python scripts from pmlb import fetch_data # Returns a pandas DataFrame adult_data = fetch_data('adult') print(adult_data.describe()) The fetch_data function has two additional parameters: * return_X_y (True/False): Whether to return the data in scikit-learn format, with the features and labels stored in separate NumPy arrays. * local_cache_dir (string): The directory on your local machine to store the data files so you don't have to fetch them over the web again. By default, the wrapper does not use a local cache directory. For example: from pmlb import fetch_data # Returns NumPy arrays adult_X, adult_y = fetch_data('adult', return_X_y=True, local_cache_dir='./') print(adult_X) print(adult_y) You can also list all of the available data sets as follows: from pmlb import dataset_names print(dataset_names) Or if you only want a list of available classification or regression datasets: from pmlb import classification_dataset_names, regression_dataset_names print(classification_dataset_names) print('') print(regression_dataset_names) Example usage: Compare two classification algorithms with PMLB PMLB is designed to make it easy to benchmark machine learning algorithms against each other. Below is a Python code snippet showing the most basic way to use PMLB to compare two algorithms. from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sb from pmlb import fetch_data, classification_dataset_names logit_test_scores = [] gnb_test_scores = [] for classification_dataset in classification_dataset_names: X, y = fetch_data(classification_dataset, return_X_y=True) train_X, test_X, train_y, test_y = train_test_split(X, y) logit = LogisticRegression() gnb = GaussianNB() logit.fit(train_X, train_y) gnb.fit(train_X, train_y) logit_test_scores.append(logit.score(test_X, test_y)) gnb_test_scores.append(gnb.score(test_X, test_y)) sb.boxplot(data=[logit_test_scores, gnb_test_scores], notch=True) plt.xticks([0, 1], ['LogisticRegression', 'GaussianNB']) plt.ylabel('Test Accuracy')","title":"Using PMLB python wrapper"},{"location":"using/#using-pmlb-in-python-scripts","text":"from pmlb import fetch_data # Returns a pandas DataFrame adult_data = fetch_data('adult') print(adult_data.describe()) The fetch_data function has two additional parameters: * return_X_y (True/False): Whether to return the data in scikit-learn format, with the features and labels stored in separate NumPy arrays. * local_cache_dir (string): The directory on your local machine to store the data files so you don't have to fetch them over the web again. By default, the wrapper does not use a local cache directory. For example: from pmlb import fetch_data # Returns NumPy arrays adult_X, adult_y = fetch_data('adult', return_X_y=True, local_cache_dir='./') print(adult_X) print(adult_y) You can also list all of the available data sets as follows: from pmlb import dataset_names print(dataset_names) Or if you only want a list of available classification or regression datasets: from pmlb import classification_dataset_names, regression_dataset_names print(classification_dataset_names) print('') print(regression_dataset_names)","title":"Using PMLB in Python scripts"},{"location":"using/#example-usage-compare-two-classification-algorithms-with-pmlb","text":"PMLB is designed to make it easy to benchmark machine learning algorithms against each other. Below is a Python code snippet showing the most basic way to use PMLB to compare two algorithms. from sklearn.linear_model import LogisticRegression from sklearn.naive_bayes import GaussianNB from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import seaborn as sb from pmlb import fetch_data, classification_dataset_names logit_test_scores = [] gnb_test_scores = [] for classification_dataset in classification_dataset_names: X, y = fetch_data(classification_dataset, return_X_y=True) train_X, test_X, train_y, test_y = train_test_split(X, y) logit = LogisticRegression() gnb = GaussianNB() logit.fit(train_X, train_y) gnb.fit(train_X, train_y) logit_test_scores.append(logit.score(test_X, test_y)) gnb_test_scores.append(gnb.score(test_X, test_y)) sb.boxplot(data=[logit_test_scores, gnb_test_scores], notch=True) plt.xticks([0, 1], ['LogisticRegression', 'GaussianNB']) plt.ylabel('Test Accuracy')","title":"Example usage: Compare two classification algorithms with PMLB"}]}